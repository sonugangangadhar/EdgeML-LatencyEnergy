{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10321243,"sourceType":"datasetVersion","datasetId":6390297}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import SimpleRNN, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\nimport joblib\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T00:47:44.075453Z","iopub.execute_input":"2025-01-10T00:47:44.075872Z","iopub.status.idle":"2025-01-10T00:47:44.082909Z","shell.execute_reply.started":"2025-01-10T00:47:44.075834Z","shell.execute_reply":"2025-01-10T00:47:44.081312Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\n# Function to preprocess the dataset\ndef preprocess_data(file_path, target_columns, look_back=5):\n    \"\"\"\n    Preprocess the dataset and reshape it for RNN.\n    \"\"\"\n    data = pd.read_excel(file_path)\n    data.columns = data.iloc[0]\n    data = data[1:]\n    data = data.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n    data.fillna(data.mean(), inplace=True)\n\n    X = data.drop(target_columns, axis=1).values\n    y = data[target_columns].values\n\n    # Standardize the features\n    scaler_X = StandardScaler()\n    scaler_y = StandardScaler()\n    X = scaler_X.fit_transform(X)\n    y = scaler_y.fit_transform(y)\n\n    # Create sequences for the RNN\n    X_seq, y_seq = [], []\n    for i in range(len(X) - look_back):\n        X_seq.append(X[i:i + look_back])  # Keep 3D shape for RNN\n        y_seq.append(y[i + look_back])\n    X_seq = np.array(X_seq)\n    y_seq = np.array(y_seq)\n\n    return X_seq, y_seq, scaler_X, scaler_y\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to preprocess testing data using the same scalers\ndef preprocess_test_data(file_path, target_columns, scaler_X, scaler_y, look_back=5):\n    \"\"\"\n    Preprocess the testing dataset using the provided scalers.\n    \"\"\"\n    data = pd.read_excel(file_path)\n    data.columns = data.iloc[0]\n    data = data[1:]\n    data = data.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n    data.fillna(data.mean(), inplace=True)\n\n    X = data.drop(target_columns, axis=1).values\n    y = data[target_columns].values\n\n    X = scaler_X.transform(X)  # Standardize using training scaler\n    y = scaler_y.transform(y)  # Standardize using training scaler\n\n    # Create sequences for the RNN\n    X_seq, y_seq = [], []\n    for i in range(len(X) - look_back):\n        X_seq.append(X[i:i + look_back])  # Keep 3D shape for RNN\n        y_seq.append(y[i + look_back])\n    X_seq = np.array(X_seq)\n    y_seq = np.array(y_seq)\n\n    return X_seq, y_seq\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# File paths\ntraining_file = '/kaggle/input/heterogenous-dataset/Training Dataset.xlsx'\ntesting_file = '/kaggle/input/heterogenous-dataset/Testing Dataset.xlsx'\ntarget_columns = ['Cloud_Throughput', 'Total_Energy_Consumption', 'Total_Exec_Time']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================= TRAINING PHASE =========================\nlook_back = 5  # Number of past time steps for prediction\nX_train, y_train, scaler_X, scaler_y = preprocess_data(training_file, target_columns, look_back)\n\n# Build the RNN model\nmodel = Sequential([\n    SimpleRNN(64, activation='relu', return_sequences=True, input_shape=(look_back, X_train.shape[2]), kernel_regularizer=l2(0.001)),\n    Dropout(0.3),\n    SimpleRNN(32, activation='relu', kernel_regularizer=l2(0.001)),\n    Dropout(0.3),\n    Dense(y_train.shape[1], activation='linear')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.0003), loss='mse', metrics=['mae'])\nprint(model.summary())\n\n# Callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, verbose=1)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    validation_split=0.2,\n    epochs=200,\n    batch_size=16,\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1\n)\n\n# Save the model and scalers\nmodel.save('rnn_model_fixed.keras')\njoblib.dump((scaler_X, scaler_y), 'rnn_scalers_fixed.pkl')\nprint(\"RNN model and scalers saved successfully!\")\n\n# Evaluate training performance\ny_train_pred = model.predict(X_train)\ny_train = scaler_y.inverse_transform(y_train)\ny_train_pred = scaler_y.inverse_transform(y_train_pred)\n\ntrain_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\ntrain_mae = mean_absolute_error(y_train, y_train_pred)\n\nprint(\"\\nRNN Training Metrics:\")\nprint(f\"RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================= TESTING PHASE =========================\n# Preprocess the testing data using the same scalers\nX_test, y_test = preprocess_test_data(testing_file, target_columns, scaler_X, scaler_y, look_back)\n\n# Predict and evaluate on the testing data\ny_test_pred = model.predict(X_test)\ny_test = scaler_y.inverse_transform(y_test)\ny_test_pred = scaler_y.inverse_transform(y_test_pred)\n\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\ntest_mae = mean_absolute_error(y_test, y_test_pred)\n\nprint(\"\\nRNN Testing Metrics:\")\nprint(f\"RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}\")\n\n# Save predictions\ny_test_combined = pd.DataFrame(y_test, columns=target_columns)\ny_test_combined['Pred_Cloud_Throughput'] = y_test_pred[:, 0]\ny_test_combined['Pred_Total_Energy_Consumption'] = y_test_pred[:, 1]\ny_test_combined['Pred_Total_Exec_Time'] = y_test_pred[:, 2]\n\ny_test_combined.to_excel('testing_predictions_rnn_fixed.xlsx', index=False)\nprint(\"Testing predictions saved to 'testing_predictions_rnn_fixed.xlsx'.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}